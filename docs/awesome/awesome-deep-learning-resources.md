<div class="github-widget" data-repo="guillaume-chevalier/awesome-deep-learning-resources"></div>
## [Awesome Deep Learning Resources](https://github.com/guillaume-chevalier/Awesome-Deep-Learning-Resources) [![Awesome](https://cdn.rawgit.com/sindresorhus/awesome/d7305f38d29fed78fa85652e3a63e154dd8e8829/media/badge.svg)](https://github.com/sindresorhus/awesome)

 这是我最喜欢的深度学习资源的粗略列表.  我学习如何进行深度学习对我有用，我用它来重新讨论主题或参考.
一世 （[Guillaume Chevalier](https://github.com/guillaume-chevalier)）已经建立了这个列表并仔细检查了这里列出的所有内容.




<a name="trends" />

## Trends

这是有史以来 [Google Trends](https://www.google.ca/trends/explore?date=all&q=machine%20learning,deep%20learning,data%20science,computer%20programming)，从2004年至今，2017年9月：
<p align="center">
  <img src="https://raw.githubusercontent.com/guillaume-chevalier/awesome-deep-learning-resources/master/google_trends.png" width="792" height="424" />
</p>

你可能也想看看Andrej Karpathy [new post](https://medium.com/@karpathy/a-peek-at-trends-in-machine-learning-ab8a1085a106) 关于机器学习研究的趋势.

 我相信深度学习是让计算机更像人类思考的关键，并且具有很大的潜力.  一些硬自动化任务可以轻松解决，而这在经典算法中无法实现.

 关于计算机科学硬件中指数级进展率的摩尔定律现在对GPU的影响比CPU更大，因为原子晶体管的微小物理限制.  我们正在转向并行架构
[[read more](https://www.quora.com/Does-Moores-law-apply-to-GPUs-Or-only-CPUs) ].  深度学习通过使用GPU在开发过程中利用并行体系结构.  最重要的是，深度学习算法可以使用量子计算，并在将来应用于机器 - 大脑界面.

I find that the key of intelligence and cognition is a very interesting subject to explore and is not yet well understood. Those technologies are promising.


<a name="online-classes" />

## Online Classes

- [Machine Learning by Andrew Ng on Coursera](https://www.coursera.org/learn/machine-learning) - 着名的入门级在线课程 [certificate](https://www.coursera.org/account/accomplishments/verify/DXPXHYFNGKG3) .  教授：斯坦福大学副教授Andrew Ng;  百度首席科学家;  Coursera的主席兼联合创始人.
- [Deep Learning Specialization by Andrew Ng on Coursera](https://www.coursera.org/specializations/deep-learning) - 由Andrew Ng开发的5个深度学习课程的新系列，现在使用的是Python而不是Matlab / Octave，这导致了一个 [specialization certificate](https://www.coursera.org/account/accomplishments/specialization/U7VNC3ZD9YD8).
- [Deep Learning by Google](https://www.udacity.com/course/deep-learning--ud730) - 良好的中级到高级课程涵盖高级深度学习概念，我发现一旦获得基础知识，它就有助于创造性.
- [Machine Learning for Trading by Georgia Tech](https://www.udacity.com/course/machine-learning-for-trading--ud501)   - 有趣的课程，用于获取应用于交易的机器学习的基本知识以及一些人工智能和金融概念.  我特别喜欢关于Q-Learning的部分.
- [Neural networks class by Hugo Larochelle, Université de Sherbrooke](https://www.youtube.com/playlist?list=PL6Xpj9I5qXYEcOhn7TqghAJ6NAPrNmUBH) -  Hugo Larochelle在线免费提供有关神经网络的有趣课程，但我观看了一些视频.
- [GLO-4030/7030 Apprentissage par réseaux de neurones profonds](https://ulaval-damas.github.io/glo4030/)   - 这是拉瓦尔大学教授PhilippeGiguère的课程.  我特别发现了令人敬畏的多头注意机制的罕见可视化，可以在其中考虑 [slide 28 of week 13's class](http://www2.ift.ulaval.ca/~pgiguere/cours/DeepLearning/09-Attention.pdf).

<a name="books" />

## Books

- [How to Create a Mind](https://www.amazon.com/How-Create-Mind-Thought-Revealed/dp/B009VSFXZ4)   - 通勤时听音频版很好听.  本书激励人们对思维进行逆向工程，并思考如何编写AI代码.
- [Neural Networks and Deep Learning](http://neuralnetworksanddeeplearning.com/index.html) - 本书涵盖了神经网络和深度学习背后的许多核心概念.
- [Deep Learning - An MIT Press book](http://www.deeplearningbook.org/) - 在本书的中途，它包含了关于如何思考实际深度学习的令人满意的数学内容.
- [Some other books I have read](https://books.google.ca/books?hl=en&as_coll=4&num=100&uid=103409002069648430166&source=gbs_slider_cls_metadata_4_mylibrary_title) - 这里列出的一些书与深度学习的关联性较小，但仍然与此列表有某种关联.

<a name="posts-and-articles" />

## Posts and Articles

- [Predictions made by Ray Kurzweil](https://en.wikipedia.org/wiki/Predictions_made_by_Ray_Kurzweil) -  Ray Kurzweil制作的中长期未来预测清单.
- [The Unreasonable Effectiveness of Recurrent Neural Networks](http://karpathy.github.io/2015/05/21/rnn-effectiveness/) - 必须阅读Andrej Karpathy的帖子 - 这是我学习RNN的动力，它展示了它在最基本的NLP形式中可以实现的目标.
- [Neural Networks, Manifolds, and Topology](http://colah.github.io/posts/2014-03-NN-Manifolds-Topology/) - 重新审视神经元如何映射信息.
- [Understanding LSTM Networks](http://colah.github.io/posts/2015-08-Understanding-LSTMs/) - 解释LSTM细胞的内部运作，并且结论中有一些有趣的联系.
- [Attention and Augmented Recurrent Neural Networks](http://distill.pub/2016/augmented-rnns/) - 对于视觉动画很有趣，作为一个例子，它是一个很好的介绍注意机制.
- [Recommending music on Spotify with deep learning](http://benanne.github.io/2014/08/05/spotify-cnns.html) - 很棒的音频聚类 - 由Spotify的实习生发布.
- [Announcing SyntaxNet: The World’s Most Accurate Parser Goes Open Source](https://research.googleblog.com/2016/05/announcing-syntaxnet-worlds-most.html) -  Parsey McParseface的诞生，一个神经语法树解析器.
- [Improving Inception and Image Classification in TensorFlow](https://research.googleblog.com/2016/08/improving-inception-and-image.html) - 非常有趣的CNN架构（例如：初始式卷积层在减少参数数量方面是有前途和有效的）.
- [WaveNet: A Generative Model for Raw Audio](https://deepmind.com/blog/wavenet-generative-model-raw-audio/) - 逼真的谈话机器：完美的声音生成.
- [François Chollet's Twitter](https://twitter.com/fchollet) -  Keras的作者 - 有趣的Twitter帖子和创新的想法.
- [Neuralink and the Brain’s Magical Future](http://waitbutwhy.com/2017/04/neuralink.html) - 引人深思的关于大脑和脑机接口未来的文章.
- [Migrating to Git LFS for Developing Deep Learning Applications with Large Files](http://vooban.com/en/tips-articles-geek-stuff/migrating-to-git-lfs-for-developing-deep-learning-applications-with-large-files/) - 轻松管理私人Git项目中的大文件.
- [The future of deep learning](https://blog.keras.io/the-future-of-deep-learning.html) - FrançoisChollet关于深度学习未来的思考.
- [Discover structure behind data with decision trees](http://vooban.com/en/tips-articles-geek-stuff/discover-structure-behind-data-with-decision-trees/) - 增加决策树并将其可视化，推断数据背后隐藏的逻辑.
- [Hyperopt tutorial for Optimizing Neural Networks’ Hyperparameters](http://vooban.com/en/tips-articles-geek-stuff/hyperopt-tutorial-for-optimizing-neural-networks-hyperparameters/) - 学会自动而不是手工杀死超参数空间.
- [Estimating an Optimal Learning Rate For a Deep Neural Network](https://medium.com/@surmenok/estimating-optimal-learning-rate-for-a-deep-neural-network-ce32f2556ce0) - 在任何单次完整训练之前估算最佳学习率的聪明技巧.
 - [The Annotated Transformer](http://nlp.seas.harvard.edu/2018/04/03/attention.html) - 有助于理解“所有你需要的注意力”（AIAYN）论文. 
 - [The Illustrated Transformer](http://jalammar.github.io/illustrated-transformer/) - 也有助于理解“所有你需要的注意力”（AIAYN）论文.
 - [Improving Language Understanding with Unsupervised Learning](https://blog.openai.com/language-unsupervised/) -  SOTA跨越许多NLP任务，来自无人监督的大型语料库预训练.
 - [NLP's ImageNet moment has arrived](https://thegradient.pub/nlp-imagenet/) - 所有冰雹NLP的ImageNet时刻. 
 - [The Illustrated BERT, ELMo, and co. (How NLP Cracked Transfer Learning)](https://jalammar.github.io/illustrated-bert/) - 了解NLP ImageNet时刻的不同方法. 
 
<a name="practical-resources" />

## Practical Resources

<a name="librairies-and-implementations" />

### Librairies and Implementations
- [TensorFlow's GitHub repository](https://github.com/tensorflow/tensorflow) - 最知名的深度学习框架，包括高级和低级，同时保持灵活性.
- [skflow](https://github.com/tensorflow/skflow) -  TensorFlow包装器 -  scikit-learn.
- [Keras](https://keras.io/) -  Keras是另一个深度学习框架，如TensorFlow，它主要是高级的.
- [carpedm20's repositories](https://github.com/carpedm20) - 许多有趣的神经网络架构由韩国人Taehoon Kim，AKA carpedm20实施.
- [carpedm20/NTM-tensorflow](https://github.com/carpedm20/NTM-tensorflow) - 神经图灵机TensorFlow实现.
- [Deep learning for lazybones](http://oduerr.github.io/blog/2016/04/06/Deep-Learning_for_lazybones) -  TensorFlow中的转学习教程，用于从预训练的CNN（AlexNet 2012）的高级嵌入中获得愿景.
- [LSTM for Human Activity Recognition (HAR)](https://github.com/guillaume-chevalier/LSTM-Human-Activity-Recognition) - 关于在时间序列上使用LSTM进行分类的我的教程.
- [Deep stacked residual bidirectional LSTMs for HAR](https://github.com/guillaume-chevalier/HAR-stacked-residual-bidir-LSTMs) - 对以前项目的改进.
- [Sequence to Sequence (seq2seq) Recurrent Neural Network (RNN) for Time Series Prediction](https://github.com/guillaume-chevalier/seq2seq-signal-prediction) - 关于如何预测数字时间序列的教程 - 可能是多通道的.
- [Hyperopt for a Keras CNN on CIFAR-100](https://github.com/Vooban/Hyperopt-Keras-CNN-CIFAR-100) - 在CIFAR-100数据集上自动（元）优化神经网络（及其架构）.
- [ML / DL repositories I starred](https://github.com/guillaume-chevalier?direction=desc&page=1&q=machine+OR+deep+OR+learning+OR+rnn+OR+lstm+OR+cnn&sort=stars&tab=stars&utf8=%E2%9C%93) -  GitHub充满了很好的代码示例和项目.
- [Smoothly Blend Image Patches](https://github.com/Vooban/Smoothly-Blend-Image-Patches) - 顺利补丁合并 [semantic segmentation with a U-Net](https://vooban.com/en/tips-articles-geek-stuff/satellite-image-segmentation-workflow-with-u-net/).
- [Self Governing Neural Networks (SGNN): the Projection Layer](https://github.com/guillaume-chevalier/SGNN-Self-Governing-Neural-Networks-Projection-Layer) - 通过这种方式，您可以在深度学习模型中使用单词而无需培训或加载嵌入.

<a name="some-datasets" />

### Some Datasets

这些是我发现的资源，这些资源似乎很有趣.

- [UCI Machine Learning Repository](https://archive.ics.uci.edu/ml/datasets.html) -  ML的数据集.
- [Cornell Movie--Dialogs Corpus](http://www.cs.cornell.edu/~cristian/Cornell_Movie-Dialogs_Corpus.html) - 这可以用于聊天机器人.
- [SQuAD The Stanford Question Answering Dataset](https://rajpurkar.github.io/SQuAD-explorer/) - 可以在线探索的问答数据集，以及在该数据集上表现良好的模型列表.
- [LibriSpeech ASR corpus](http://www.openslr.org/12/) - 具有平衡性别和扬声器的巨大免费英语语音数据集，似乎是高质量的.
- [Awesome Public Datasets](https://github.com/caesar0301/awesome-public-datasets) - 一个很棒的公共数据集列表.
- [SentEval: An Evaluation Toolkit for Universal Sentence Representations](https://arxiv.org/abs/1803.05449) - 一个Python框架，用于对许多数据集（NLP任务）上的句子表示进行基准测试. 
- [ParlAI: A Dialog Research Software Platform](https://arxiv.org/abs/1705.06476) - 另一个Python框架，用于对许多数据集（NLP任务）上的句子表示进行基准测试.


<a name="other-math-theory" />

## Other Math Theory

<a name="gradient-descent-algorithms-and-optimization" />

### Gradient Descent Algorithms & Optimization Theory

- [Neural Networks and Deep Learning, ch.2](http://neuralnetworksanddeeplearning.com/chap2.html) - 关于反向传播算法如何工作的概述.
- [Neural Networks and Deep Learning, ch.4](http://neuralnetworksanddeeplearning.com/chap4.html) - 神经网络可以计算任何功能的视觉证据.
- [Yes you should understand backprop](https://medium.com/@karpathy/yes-you-should-understand-backprop-e2f06eab496b#.mr5wq61fb) - 揭示backprop的警告以及在训练模型时知道这一点的重要性.
- [Artificial Neural Networks: Mathematics of Backpropagation](http://briandolhansky.com/blog/2013/9/27/artificial-neural-networks-backpropagation-part-4) - 以数学方式描绘背景.
- [Deep Learning Lecture 12: Recurrent Neural Nets and LSTMs](https://www.youtube.com/watch?v=56TYLaQN4N8) - 正确解释了RNN图的展开，并揭示了关于梯度下降算法的潜在问题.
- [Gradient descent algorithms in a saddle point](http://sebastianruder.com/content/images/2016/09/saddle_point_evaluation_optimizers.gif) - 可视化不同的优化器如何与鞍点相互作用.
- [Gradient descent algorithms in an almost flat landscape](https://devblogs.nvidia.com/wp-content/uploads/2015/12/NKsFHJb.gif) - 可视化不同的优化器如何与几乎平坦的景观进行交互.
- [Gradient Descent](https://www.youtube.com/watch?v=F6GSRDoB-Cg) - 好的，我已经在上面列出了Andrew NG的Coursera课程，但是这个视频特别适合作为介绍并定义了梯度下降算法.
- [Gradient Descent: Intuition](https://www.youtube.com/watch?v=YovTqTY-PYY) - 从上一个视频得到的内容：现在添加直觉.
- [Gradient Descent in Practice 2: Learning Rate](https://www.youtube.com/watch?v=gX6fZHgfrow) - 如何调整神经网络的学习率.
- [The Problem of Overfitting](https://www.youtube.com/watch?v=u73PU6Qwl1I) - 对过度拟合以及如何解决该问题的一个很好的解释.
- [Diagnosing Bias vs Variance](https://www.youtube.com/watch?v=ewogYw5oCAI) - 了解神经网络预测中的偏差和方差以及如何解决这些问题.
- [Self-Normalizing Neural Networks](https://arxiv.org/pdf/1706.02515.pdf) - 出现令人难以置信的SELU激活功能.
- [Learning to learn by gradient descent by gradient descent](https://arxiv.org/pdf/1606.04474.pdf) -  RNN作为优化器：引入L2L优化器，一个元神经网络.

<a name="complex-numbers-and-digital-signal-processing" />

### Complex Numbers & Digital Signal Processing

好的，信号处理可能与深度学习没有直接关系，但研究它对于开发基于信号的神经架构有更多的直觉是有趣的.

- [Window Functions](https://en.wikipedia.org/wiki/Window_function) - 维基百科页面列出了一些已知的窗口函数 - 请注意 [Hann-Poisson window](https://en.wikipedia.org/wiki/Window_function#Hann%E2%80%93Poisson_window) 对于贪婪的爬山算法（例如梯度下降）特别有趣. 
- [MathBox, Tools for Thought Graphical Algebra and Fourier Analysis](https://acko.net/files/gltalks/toolsforthought/) - 傅立叶分析的新面貌.
- [How to Fold a Julia Fractal](http://acko.net/blog/how-to-fold-a-julia-fractal/) - 处理复数和波动方程的动画.
- [Animate Your Way to Glory, Math and Physics in Motion](http://acko.net/blog/animate-your-way-to-glory/) - 物理引擎中的收敛方法，并应用于交互设计.
- [Animate Your Way to Glory - Part II, Math and Physics in Motion](http://acko.net/blog/animate-your-way-to-glory-pt2/) - 使用四元数（用于处理3D旋转的数学对象）进行旋转和旋转插值的精美动画.
- [Filtering signal, plotting the STFT and the Laplace transform](https://github.com/guillaume-chevalier/filtering-stft-and-laplace-transform) - 关于信号处理的简单Python演示.


<a name="papers" />

## Papers

<a name="recurrent-neural-networks" />

### Recurrent Neural Networks

- [Deep Learning in Neural Networks: An Overview](https://arxiv.org/pdf/1404.7828v4.pdf) -  You_Again的深度学习摘要/概述，主要是关于RNN.
- [Bidirectional Recurrent Neural Networks](http://www.di.ufpe.br/~fnj/RNA/bibliografia/BRNN.pdf) - 使用在时间轴上进行双向扫描的RNN进行更好的分类.
- [Learning Phrase Representations using RNN Encoder-Decoder for Statistical Machine Translation](https://arxiv.org/pdf/1406.1078v3.pdf)   - 两个网络合二为一的seq2seq（序列到序列）编码器 - 解码器架构.  RNN编码器 - 解码器，具有1000个隐藏单元.  Adadelta优化器.
- [Sequence to Sequence Learning with Neural Networks](http://papers.nips.cc/paper/5346-sequence-to-sequence-learning-with-neural-networks.pdf) - 在WMT&#39;14英语到法语数据集上，4个堆叠的LSTM单元，1000个隐藏大小，带有反向输入句子，并带有光束搜索.
- [Exploring the Limits of Language Modeling](https://arxiv.org/pdf/1602.02410.pdf) - 使用过量的GPU功率在字符级CNN上使用字级LSTM的漂亮递归模型.
- [Neural Machine Translation and Sequence-to-sequence Models: A Tutorial](https://arxiv.org/pdf/1703.01619.pdf) - 对NMT主题的有趣概述，我主要阅读关于RNN的第8部分，并将其作为回顾.
- [Exploring the Depths of Recurrent Neural Networks with Stochastic Residual Learning](https://cs224d.stanford.edu/reports/PradhanLongpre.pdf) - 基本上，在所呈现的情绪分析情况下，残余连接可能优于堆叠的RNN.
- [Pixel Recurrent Neural Networks](https://arxiv.org/pdf/1601.06759.pdf) - 很适合类似Photoshop的“内容识别填充”来填充图像中缺失的补丁.
- [Adaptive Computation Time for Recurrent Neural Networks](https://arxiv.org/pdf/1603.08983v4.pdf)   - 让RNN决定他们计算的时间.  我很想看看它与神经图灵机的结合程度如何.  可以找到关于该主题的有趣的交互式可视化 [here](http://distill.pub/2016/augmented-rnns/).

<a name="convolutional-neural-networks" />

### Convolutional Neural Networks

- [What is the Best Multi-Stage Architecture for Object Recognition?](http://yann.lecun.com/exdb/publis/pdf/jarrett-iccv-09.pdf) - 使用“局部对比度标准化”非常棒.
- [ImageNet Classification with Deep Convolutional Neural Networks](http://www.cs.toronto.edu/~fritz/absps/imagenet.pdf) -  AlexNet，2012 ILSVRC，ReLU激活功能的突破.
- [Visualizing and Understanding Convolutional Networks](https://arxiv.org/pdf/1311.2901v3.pdf) - 对于“deconvnet图层”.
- [Fast and Accurate Deep Network Learning by Exponential Linear Units](https://arxiv.org/pdf/1511.07289v1.pdf) - 用于CIFAR视觉任务的ELU激活功能.
- [Very Deep Convolutional Networks for Large-Scale Image Recognition](https://arxiv.org/pdf/1409.1556v6.pdf)   - 有趣的想法是在汇集多个3x3 conv + ReLU之前，只需几个参数就可以获得更大的滤波器尺寸.  “ConvNet配置”还有一个很好的表格.
- [Going Deeper with Convolutions](http://www.cv-foundation.org/openaccess/content_cvpr_2015/papers/Szegedy_Going_Deeper_With_2015_CVPR_paper.pdf) -  GoogLeNet：“初始”层/模块的外观，其思想是将conv层并行化为许多不同大小的mini-conv，并使用“相同”填充，在深度上连接.
- [Highway Networks](https://arxiv.org/pdf/1505.00387v2.pdf) - 公路网：剩余连接.
- [Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift](https://arxiv.org/pdf/1502.03167v3.pdf) - 批量标准化（BN）：通过对整个批次求和，然后执行线性重新缩放和移动某个可训练量来标准化层的输出.
- [U-Net: Convolutional Networks for Biomedical Image Segmentation](https://arxiv.org/pdf/1505.04597.pdf) -  U-Net是一种编码器 - 解码器CNN，它也具有跳过连接，适用于每像素级别的图像分割.
- [Deep Residual Learning for Image Recognition](https://arxiv.org/pdf/1512.03385v1.pdf) - 具有批量标准化层的非常深的残余层 - 也称为“如何过度填充具有太多层的任何视觉数据集，并且在给定足够数据的情况下使任何视觉模型在识别时正常工作”.
- [Inception-v4, Inception-ResNet and the Impact of Residual Connections on Learning](https://arxiv.org/pdf/1602.07261v2.pdf) - 使用剩余连接改进GoogLeNet.
- [WaveNet: a Generative Model for Raw Audio](https://arxiv.org/pdf/1609.03499v2.pdf) - 基于扩张的因果卷积的新架构的史诗原始语音/音乐生成，以捕获更多的音频长度.
- [Learning a Probabilistic Latent Space of Object Shapes via 3D Generative-Adversarial Modeling](https://arxiv.org/pdf/1610.07584v2.pdf) - 用于3D模型生成的3D-GAN和来自嵌入的有趣3D家具算法（想像word2vec单词算术与3D家具表示）.
- [Accurate, Large Minibatch SGD: Training ImageNet in 1 Hour](https://research.fb.com/publications/ImageNet1kIn1h/) - 令人难以置信的快速分布式CNN培训.
- [Densely Connected Convolutional Networks](https://arxiv.org/pdf/1608.06993.pdf) -  2017年CVPR最佳论文奖，对CIFAR-10，CIFAR-100和SVHN数据集的最新表现进行了改进，这种新的神经网络架构被命名为DenseNet.
- [The One Hundred Layers Tiramisu: Fully Convolutional DenseNets for Semantic Segmentation](https://arxiv.org/pdf/1611.09326.pdf) - 融合了U-Net和DenseNet的思想，这种新的神经网络特别适用于图像分割中的大型数据集.
- [Prototypical Networks for Few-shot Learning](https://arxiv.org/pdf/1703.05175.pdf) - 在一些示例中，使用损失中的距离度量来确定对象属于哪个类.

<a name="attention-mechanisms" />

### Attention Mechanisms

- [Neural Machine Translation by Jointly Learning to Align and Translate](https://arxiv.org/pdf/1409.0473.pdf)   -  LSTM的注意机制！  大多数情况下，数字和公式及其解释显示对我有用.  我在那篇论文上发表了演讲 [here](https://www.youtube.com/watch?v=QuvRWevJMZ4).
- [Neural Turing Machines](https://arxiv.org/pdf/1410.5401v2.pdf)   - 出色的让神经网络学习一种算法，在长时间的依赖性上看似很好的泛化.  序列召回问题.
- [Show, Attend and Tell: Neural Image Caption Generation with Visual Attention](https://arxiv.org/pdf/1502.03044.pdf) -  LSTM对CNN功能图的关注机制确实很奇怪.
- [Teaching Machines to Read and Comprehend](https://arxiv.org/pdf/1506.03340v3.pdf) - 关于文本问题回答的非常有趣和创造性的工作，有什么突破，与此有关.
- [Effective Approaches to Attention-based Neural Machine Translation](https://arxiv.org/pdf/1508.04025.pdf) - 探索注意机制的不同方法.
- [Matching Networks for One Shot Learning](https://arxiv.org/pdf/1606.04080.pdf) - 通过使用注意机制和查询将图像与其他图像进行比较以进行分类，以低数据进行一次性学习的有趣方式.
- [Google’s Neural Machine Translation System: Bridging the Gap between Human and Machine Translation](https://arxiv.org/pdf/1609.08144.pdf) -  2016年：在编码器/解码器上具有注意机制的堆叠残余LSTM对于NMT（神经机器翻译）是最佳的.
- [Hybrid computing using a neural network with dynamic external memory](http://www.nature.com/articles/nature20101.epdf?author_access_token=ImTXBI8aWbYxYQ51Plys8NRgN0jAjWel9jnR3ZoTv0MggmpDmwljGswxVdeocYSurJ3hxupzWuRNeGvvXnoO8o4jTJcnAyhGuZzXJ1GEaD-Z7E6X_a9R-xqJ9TfJWBqz) - 基于NTM的可微分存储器的改进：现在它是可微分神经计算机（DNC）.
- [Massive Exploration of Neural Machine Translation Architectures](https://arxiv.org/pdf/1703.03906.pdf) - 这使得对于在框架式seq2seq问题公式中进行NMT的边界有直觉.
-  [通过在Mel谱图上调节WaveNet的自然TTS合成
预测]（https://arxiv.org/pdf/1712.05884.pdf） -  A. [WaveNet](https://arxiv.org/pdf/1609.03499v2.pdf) 用作声码器的条件可以是来自Tacotron 2 LSTM神经网络的生成的Mel谱图，注意从文本生成整齐的音频.
- [Attention Is All You Need](https://arxiv.org/abs/1706.03762) （AIAYN） - 引入具有位置编码的多头自我关注神经网络来完成没有任何RNN或CNN的句子级NLP  - 本文是必读的（另见 [this explanation](http://nlp.seas.harvard.edu/2018/04/03/attention.html) 和 [this visualization](http://jalammar.github.io/illustrated-transformer/) 该论文）. 

<a name="other" />

### Other

- [ProjectionNet: Learning Efficient On-Device Deep Networks Using Neural Projections](https://arxiv.org/abs/1708.00630) - 通过深度神经网络中的单词投影替换单词嵌入，这不需要预先提取的字典也不需要存储嵌入矩阵. 
- [Self-Governing Neural Networks for On-Device Short Text Classification](http://aclweb.org/anthology/D18-1105)   - 本文是上面ProjectionNet的续集.  SGNN在ProjectionNet上进行了详细阐述，并且更加详细地详述了优化（另请参阅我 [attempt to reproduce the paper in code](https://github.com/guillaume-chevalier/SGNN-Self-Governing-Neural-Networks-Projection-Layer) 并观看 [the talks' recording](https://vimeo.com/305197775)).
- [Matching Networks for One Shot Learning](https://arxiv.org/abs/1606.04080)   - 从其他示例列表（没有明确的类别）和每个分类任务的低数据分类新示例，但许多类似的分类任务的大量数据 - 它似乎比暹罗网络更好.  总结一下：使用匹配网络，您可以直接优化示例之间的余弦相似性（如自我关注产品匹配），直接传递给softmax.  我想匹配网络可能可以用作word2vec的CBOW或Skip-gram中的负采样softmax训练，而无需进行任何上下文嵌入查找. 


<a name="youtube" />

## YouTube and Videos

- [Attention Mechanisms in Recurrent Neural Networks (RNNs) - IGGG](https://www.youtube.com/watch?v=QuvRWevJMZ4) - 关于注意力机制的阅读小组的讨论（论文：通过共同学习对齐和翻译的神经机器翻译）.
- [Tensor Calculus and the Calculus of Moving Surfaces](https://www.youtube.com/playlist?list=PLlXfTHzgMRULkodlIEqfgTS-H1AY_bNtq) - 正确概括Tensors如何工作，但只是观看一些视频已经有助于掌握概念.
- [Deep Learning & Machine Learning (Advanced topics)](https://www.youtube.com/playlist?list=PLlp-GWNOd6m4C_-9HxuHg2_ZeI2Yzwwqt) - 我发现有趣或有用的深度学习视频列表，这是一些东西的混合.
- [Signal Processing Playlist](https://www.youtube.com/playlist?list=PLlp-GWNOd6m6gSz0wIcpvl4ixSlS-HEmr) - 我编写的关于DFT / FFT，STFT和拉普拉斯变换的YouTube播放列表 - 我对我的软件工程学士学位不满，不包括信号处理课程（量子物理课除外）.
- [Computer Science](https://www.youtube.com/playlist?list=PLlp-GWNOd6m7vLOsW20xAJ81-65C-Ys6k) - 我编写的另一个YouTube播放列表，这次是关于各种CS主题.
- [Siraj's Channel](https://www.youtube.com/channel/UCWN3xxRkmTPmbKwht9FuE5A/videos?view=0&sort=p&flow=grid) -  Siraj提供有关深度学习的有趣，快节奏的视频教程.
- [Two Minute Papers' Channel](https://www.youtube.com/user/keeroyz/videos?sort=p&view=0&flow=grid) - 一些研究论文的有趣和浅薄的概述，例如关于WaveNet或神经样式转移.
- [Geoffrey Hinton interview](https://www.coursera.org/learn/neural-networks-deep-learning/lecture/dcm5r/geoffrey-hinton-interview) -  Andrew Ng采访了Geoffrey Hinton，他谈论了他的研究和突破，并为学生提供建议.

<a name="misc-hubs-and-links" />

## Misc. Hubs & Links

- [Hacker News](https://news.ycombinator.com/news) - 也许我是如何发现ML的 - 有趣的趋势出现在该网站的方式之前，他们成为一个大问题.
- [DataTau](http://www.datatau.com/) - 这是一个类似于黑客新闻的中心，但特定于数据科学.
- [Naver](http://www.naver.com/)   - 这是一个韩国搜索引擎 - 最适合谷歌翻译使用，具有讽刺意味.  令人惊讶的是，有时深度学习搜索结果和可理解的高级数学内容比Google搜索更容易显示.
- [Arxiv Sanity Preserver](http://www.arxiv-sanity.com/) - 具有TF / IDF功能的arXiv浏览器.


<a name="license" />

## License

[![CC0](http://mirrors.creativecommons.org/presskit/buttons/88x31/svg/cc-zero.svg)](https://creativecommons.org/publicdomain/zero/1.0/)

在法律允许的范围内， [Guillaume Chevalier](https://github.com/guillaume-chevalier) 已放弃对此作品的所有版权及相关或相邻权利.
